# -*- coding: utf-8 -*-
"""Projeto_Embraer_ICD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E35z7qK6sWOZtrI12RcAI69DMIQiSYvh

# Projeto Embraer - Indrodução Ciência de Dados
# Importação dos Pacotes
# Acesso aos Dados
# Automatização do dadaset analístico
# Fazendo uma previsão de venda
"""

# Commented out IPython magic to ensure Python compatibility.
# Importação dos Pacotes :
# importação do pacote pandas de manipulação de dados
import pandas as pd
# pacote de funções matemáticas
import numpy as np
 # Gráficos
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')
# pacote de visualização de dados
import matplotlib.pyplot as plt
# pacote acesso aos dados
import gdown

# %matplotlib inline

# Biblioteca Python para Realizar o acesso aos Dados
!pip install gdown

# Carregando o arquivo excel
carros = pd.read_excel("preco-carros.xlsx")
# Lendo Dataset
print(carros)

# Automatização de análise do dataset analístico
!pip install pandas-profiling

import pandas as pd
from pandas_profiling import ProfileReport
from IPython.display import display, HTML

# Crie o relatório de perfil usando o Pandas Profiling
profile = ProfileReport(carros)

# Salve o relatório em HTML
profile.to_file("output.html")

# Exiba o relatório de perfil
display(HTML(profile.to_html()))

# Carregando o arquivo
carros = pd.read_excel('preco-carros.xlsx')

# Mostrando as primeiras linhas do dataframe
carros.head()

# Removendo linhas duplicadas
carros = carros.drop_duplicates()
print(carros)

# Verificar as Variáveis (colunas-campos-features) :
carros.columns

# Escolher as Variáveis e salvar no dataset:
dataset = carros[['ano', 'preco_venda', 'preco_atu', 'kms_rodado',
       'combustivel', 'tipo_vendendor', 'transmissao', 'proprietario']]

# Visualizar o data set :
dataset.head()

# Criação de um campo com o ano corrente:
dataset['ano_atual'] = 2023

# Criar a nova coluna - Idade do carro
dataset['idade_carro'] = dataset['ano_atual']-dataset['ano']

# Visualizar o dataset
dataset.head()

# Eliminar (dropar) as coolunas desnecessárias :
dataset.drop(['ano', 'ano_atual'],axis =1, inplace = True)

# Transformar campos textuais em numéricos - OneHotCoding :
dataset = pd.get_dummies(dataset,drop_first = True)

dataset.head()

#Separação das variáveis explicativa e o Target (variável que queremos prever) :
X = dataset.iloc[:,1:]
y = dataset.iloc[:,0]

# Separação das amostras para treinamento e teste da Máquina Preditiva - Train Test Split :
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)

# Lendo treinamento
X_train

# Lendo teste
X_test

# Utilização do Algoritmo RandomForestRegressor  para criação da Máquina Preditiva:
from sklearn.ensemble import RandomForestRegressor
rf_random = RandomForestRegressor()

#Treinamento da MP com os dados de treino
rf_random.fit(X_train,y_train)

# Teste da MP com novos Dados
predictions = rf_random.predict(X_test)

# Lendo previsões
predictions

# Avaliação da Máquina Preditiva
sns.distplot(y_test-predictions)

# Salvamento da MP como arquivo "pkl"
import pickle
file = open('Random_forest_regression_model.pkl','wb')
pickle.dump(rf_random,file)

"""# Projeto Parte I
# Preparando para fazer teste para Regressão Simples
"""

#Regressão linear simples
import numpy as np
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
print(carros.describe())

#Primeiro vamos criar uma plotagem para entendermos o comportamento do que é preco_atu com relação ao retorno de preco_venda:
plt.figure(figsize = (16,8))
plt.scatter(
    carros['preco_atu'],
    carros['preco_venda'],
    c='red')
plt.xlabel(" ($) Preco Atual do Automóvel")
plt.ylabel(" ($) Preco Venda do Automóvel")

#Como você pode observar, existe uma relação clara entre o preco_atu e o retorno em preco_venda.
#Agora vamos criar o modelo para prever o quanto teremos de retorno com o investimento em preco_atu:
X = carros['preco_atu'].values.reshape(-1,1)
y = carros['preco_venda'].values.reshape(-1,1)


reg = LinearRegression()
reg.fit(X, y)

# O "X" representa o valor investido em preco_atu, ou seja, essa é a conta matemática que nos ajuda a prever o retorno de um investimento!

# Agora vamos plotar o nosso modelo em cima dos dados e analisar se ele é bom, ruim, explica muito ou explica pouco o comportamento de nossos dados:
print("O modelo é: Vendas = {:.5} + {:.5}X".format(reg.intercept_[0], reg.coef_[0][0]))

# Avaliando a precisão do modelo:
# Pontos vermelhos: são os valores reais do nosso conjunto de dados
# Linha azul: ela representa os erros entre os valores reais e os valores preditos pelo nosso modelo
f_previsaoes = reg.predict(X)


plt.figure(figsize = (16,8))
plt.scatter(
    carros['preco_atu'],
    carros['preco_venda'],
    c='red')


plt.plot(
    carros['preco_atu'],
    f_previsaoes,
    c='blue',
    linewidth=3,
    linestyle=':'
)

plt.xlabel(" ($) Preco Atual do Automóvel")
plt.ylabel(" ($) Preco Venda do Automóve")
plt.show()

# Com o código a baixo criamos um resumo que mostra várias características de nosso modelo. Avaliaremos sua qualidade através do R² e do "p-valor":
# Observe que o R² está em 0.161, isso quer dizer que aproximadamente 16% do comportamento da variável "Preco Vendas" é explicado pela variável "Preco Atual".
# Quando analisamos o "p-valor" observamos que ele está bem baixo, algo bem próximo à zero foi encoberto pelo arredondamento, isso nos faz rejeitar a hipótese nula.
X = carros['preco_atu']
y = carros['preco_venda']
X2 = sm.add_constant(X)
est = sm.OLS(y, X2)
est2 = est.fit()
print(est2.summary())

# Setando campos como categóricos
carros.dtypes

#ajustando tipos dos dados
carros['modelo'] = carros['modelo'].astype('category')
carros['combustivel'] = carros['combustivel'].astype('category')
carros['tipo_vendendor'] = carros['tipo_vendendor'].astype('category')
carros['transmissao'] = carros['transmissao'].astype('category')

carros.dtypes

carros['modelo'].cat.categories

carros['transmissao'].cat.categories

carros['transmissao'] = carros['transmissao'].cat.codes

carros.head()

carros.dtypes

# Removendo instâncias
carros.shape

print(carros.isnull().sum())

# remove instâncias com ao menos 1 coluna com dado ausente
carros1 = carros.dropna()
carros1.head()

len(carros1)

print(carros1.isnull().sum())

# Removendo colunas quase vaziais
print(carros.isnull().sum())

#remove colunas com ao menos 70% de dados ausentes
carros2 = carros.dropna(thresh=0.7*len(carros), axis=1)

carros2.head()

print(carros2.isnull().sum())

# Inserindo dados ausentes em 1 coluna
print(carros2['preco_atu'].mean())
print(carros2['preco_atu'].median())

#adiciona media de preco_atu nos dados ausentes de preco_atu
carros2['preco_atu'].fillna(carros2['preco_atu'].mean(), inplace=True)

print(carros2.isnull().sum())

print(carros2['preco_atu'].mean())
print(carros2['preco_atu'].median())

carros2.corr(method='spearman')

carros.corr(method='spearman')

#remove colunas com até 70% de dados ausentes
carros2 = carros.dropna(thresh=0.7*len(carros), axis=1)
#adiciona mediana de preco_atu nos dados ausentes de preco_atu
carros2['preco_atu'].fillna(carros2['preco_atu'].median(), inplace=True)

print(carros2['preco_atu'].mean())
print(carros2['preco_atu'].median())

carros.corr(method='spearman')

carros2.corr(method='spearman')

carros['preco_atu'].describe()

import random
carros2 = carros.dropna(thresh=0.7*len(carros), axis=1)
#adiciona valor aleatório no intervalo de min e max de preco_atu nos dados ausentes de preco_atu
carros2['preco_atu'].fillna(random.gammavariate(carros2['preco_atu'].min(),carros2['preco_atu'].max()), inplace=True)

print(carros2['preco_atu'].mean())
print(carros2['preco_atu'].median())

# Inserindo dados ausentes em mais de uma coluna
carros = pd.read_excel("/content/preco-carros.xlsx")
carros.head()

#ajustando tipos dos dados
carros['modelo'] = carros['modelo'].astype('category')
carros['combustivel'] = carros['combustivel'].astype('category')
carros['tipo_vendendor'] = carros['tipo_vendendor'].astype('category')
carros['transmissao'] = carros['transmissao'].astype('category')

carros['modelo'] = carros['modelo'].cat.codes
carros['combustivel'] = carros['combustivel'].cat.codes
carros['tipo_vendendor'] = carros['tipo_vendendor'].cat.codes
carros['transmissao'] = carros['transmissao'].cat.codes

carros.head()

!pip install fancyimpute
from fancyimpute import KNN
carros_knn = KNN(k=3).fit_transform(carros)

carros_knn.shape

carros3 = pd.DataFrame(data=carros_knn[0:,0:],columns=['modelo',	'ano',	'preco_atu',	'preco_venda',	'kms_rodado',	'combustivel',	'tipo_vendedor',	'transmissao', 'proprietario'])

carros3.head()

print(carros3.isnull().sum())

carros3['modelo'].describe()

# Normalização
carros3.head()

#preço está dominando o cálculo
dist = np.linalg.norm(carros3.values[1]-carros3.values[2])
print(dist)

# normalizando min-max
carros_norm = (carros3 - carros3.min()) / (carros3.max() - carros3.min())
print(carros_norm.head())

carros_norm = carros_norm.dropna(thresh=0.7*len(carros), axis=1)
print(carros_norm.head())

dist = np.linalg.norm(carros_norm.values[1]-carros_norm.values[2])
print(dist)

# nomarlizando entre 0 e 1
carros['preco_venda_norm'] = (carros['preco_venda'] - carros['preco_venda'].min()) / (carros['preco_venda'].max() - carros['preco_venda'].min())

carros['preco_venda_norm'].describe()

# nomarlizando entre -1 e 1
carros['preco_venda_norm'] = ((carros['preco_venda'] - carros['preco_venda'].min()) / (carros['preco_venda'].max() - carros['preco_venda'].min())*2) -1

carros['preco_venda_norm'].describe()

# min-max do scikit
from sklearn.preprocessing import StandardScaler, MinMaxScaler
scaler = MinMaxScaler()
carros['preco_venda_norm'] = scaler.fit_transform(carros[['preco_venda']])
carros['preco_venda_norm'].describe()

# nomalizando com média e desvio padrão
carros['preco_venda_norm'] = (carros['preco_venda'] - carros['preco_venda'].mean()) / (carros['preco_venda'].std())

carros['preco_venda_norm'].describe()

# nomalizando com média e desvio padrão do scikit
scaler = StandardScaler()
carros['preco_venda_norm'] = scaler.fit_transform(carros[['preco_venda']])
carros['preco_venda_norm'].describe()

# Discretizando kms_rodado
# discretizando com 10 bins com mesmo intervalo
carros['kms_rodado_dist'] = pd.cut(carros['kms_rodado'],10)

carros['kms_rodado_dist'].value_counts()

carros['kms_rodado'].describe()

# discretizando com 4 bins com mesmo intervalo (quantiles)
carros['kms_rodado_dist'] = pd.qcut(carros['kms_rodado'],4)

carros['kms_rodado'].describe()

carros['kms_rodado_dist'].value_counts()

"""# Projeto Parte II"""

import numpy as np
import pandas as pd
carros = pd.read_excel("/content/preco-carros.xlsx")
carros = carros[carros['preco_atu'] > 200000]
print(len(carros))
print(carros.head())

carros[carros.duplicated()].sort_values("preco_atu").head()

len(carros)

# removendo entradas duplicadas
carros.drop_duplicates(keep = False, inplace = True)
print(len(carros))

carros = pd.read_excel("/content/preco-carros.xlsx")
carros = carros[carros['preco_atu'] > 200000]

# removendo baseado em um conjunto de atributos
carros.drop_duplicates(subset =['preco_atu','preco_atu'], keep = False, inplace = True)
print(len(carros))
print(carros.head())

#Removendo outliers utilizando z-score robusto (univariado)
carros = pd.read_excel("/content/preco-carros.xlsx")
carros = carros[carros['preco_atu'] > 200000]
print(len(carros))
print(carros.head())

carros['preco_venda'].describe()

carros['preco_venda'].plot.box()

carros['preco_venda'].hist()

from numpy import log10
carros['preco_venda_log'] = log10(carros['preco_venda'])
carros['preco_venda_log'].hist()

from numpy import abs
mad = abs(carros['preco_venda_log'] - carros['preco_venda_log'].median()).median()*(1/0.6745)
print(mad)

carros['preco_venda_log'].mad()

(abs(carros['preco_venda_log']-carros['preco_venda_log'].median())/mad).hist()

carros[abs(carros['preco_venda_log']-carros['preco_venda_log'].median())/mad > 3.5]

# Removendo outliers utilizando método de tukey (univariado)
carros = pd.read_excel("/content/preco-carros.xlsx")
carros = carros[carros['preco_atu'] > 200000]
print(len(carros))
print(carros.head())

carros['preco_atu'].describe()

carros['preco_atu'].plot.box()

carros['preco_atu'].hist()

carros['preco_atu_log'] = log10(carros['preco_atu'])
carros['preco_atu_log'].hist()

q1 = carros['preco_atu_log'].quantile(q=0.25)
q3 = carros['preco_atu_log'].quantile(q=0.75)
iqr = q3 - q1
print(iqr)

carros[carros['preco_atu_log'] < (q1-1.5*iqr)]

carros[carros['preco_atu_log'] > (q3+1.5*iqr)]

carros = carros[(carros['preco_atu_log'] > (q1-1.5*iqr)) & (carros['preco_atu_log'] < (q3+1.5*iqr))]
len(carros)

# Removendo outliers usando razão de duas variáveis (bivariado)
carros.plot.scatter(x='kms_rodado',y='preco_atu')

carros['preco_atu_kms'] = carros['preco_atu'] / carros['kms_rodado']

carros['preco_atu_kms'].describe()

carros['preco_atu_kms'].plot.box()

carros['preco_atu_kms'].hist()

carros['preco_atu_kms'] = log10(carros['preco_atu_kms'])

carros['preco_atu_kms'].hist()

mad = abs(carros['preco_atu_kms'] - carros['preco_atu_kms'].median()).median()*(1/0.6745)

(abs(carros['preco_atu_kms']-carros['preco_atu_kms'].median())/mad).hist()

carros[abs(carros['preco_atu_kms']-carros['preco_atu_kms'].median())/mad > 2.5]

carros = carros[abs(carros['preco_atu_kms']-carros['preco_atu_kms'].median())/mad < 3.5]
print(len(carros))

# Removendo outliers com Elliptic Envelope (multivariado)
carros = pd.read_excel("/content/preco-carros.xlsx")
carros = carros[carros['preco_atu'] > 200000]
carros = carros.dropna()
print(carros.head())

carros.dtypes

#ajustando tipos dos dados
carros['modelo'] = carros['modelo'].astype('category')
carros['combustivel'] = carros['combustivel'].astype('category')
carros['tipo_vendendor'] = carros['tipo_vendendor'].astype('category')
carros['transmissao'] = carros['transmissao'].astype('category')

carros['modelo'] = carros['modelo'].cat.codes
carros['combustivel'] = carros['combustivel'].cat.codes
carros['tipo_vendendor'] = carros['tipo_vendendor'].cat.codes
carros['transmissao'] = carros['transmissao'].cat.codes

carros.head()

from sklearn.covariance import EllipticEnvelope
detector = EllipticEnvelope(contamination=0.01)
detector.fit(carros)

scores = detector.predict(carros)

scores

carros['outlier'] = scores
print(carros.head())

carros[carros['outlier'] == -1]

carros = carros[carros['outlier'] != -1]
len(carros)

# Removendo outliers com Isolation Forests (multivariado)
carros = pd.read_excel("/content/preco-carros.xlsx")
carros = carros[carros['preco_atu'] > 200000]
carros = carros.dropna()
print(carros.head())

#ajustando tipos dos dados
carros['modelo'] = carros['modelo'].astype('category')
carros['combustivel'] = carros['combustivel'].astype('category')
carros['tipo_vendendor'] = carros['tipo_vendendor'].astype('category')
carros['transmissao'] = carros['transmissao'].astype('category')

carros['modelo'] = carros['modelo'].cat.codes
carros['combustivel'] = carros['combustivel'].cat.codes
carros['tipo_vendendor'] = carros['tipo_vendendor'].cat.codes
carros['transmissao'] = carros['transmissao'].cat.codes

from sklearn.ensemble import IsolationForest
rng = np.random.RandomState(42)
clf = IsolationForest(max_samples=100, random_state=rng)
clf.fit(carros)

scores = clf.predict(carros)

scores

carros['outlier'] = scores
print(carros.head())

carros[carros['outlier'] == -1]

carros = carros[carros['outlier'] != -1]
len(carros)

"""# Fazendo teste para expectativa da Média"""

# importar a bibliotecas
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import math # biblioteca para cálculos matemáticos
import seaborn as sns # biblioteca para apresentação visual
#configurações visuais da biblioteca seaborn para os gráficos
sns.set(rc={'figure.figsize':(10, 6)}) # tamanho da área de plotagem
sns.set_context("talk") # fontes maiores na visualização

# Lista com a expectativa MÈDIA de carro nos preço atual, preço venda e kms rodados
# cada coluna representa a média da expectativa da média em cada ano apurado de 2003 a 2018
PrecoAtualMedia = [17.424,29.736,30.702,12.935,21.413,30.640,25.310,26.837,27.710,27.803,1.122,32.354,30.982,24.032,22.473,33.632,33.632,33.632,15.865,24.873,12.312,17.798,24.967,10.784,13.745,15.553,18.296,20.229,12.312,32.354,18.639,15.242,23.346,31.014,25.123,24.125,2.244,7.106,11.719,24.873,
                  24.530,12.405,22.286,25.123,8.384,37.528,15.242,12.935,24.032,28.956,95.411,95.411,61.623,95.411,31.824,46.879,22.660,57.789,2.119,112.087,58.007,2.400,112.087,112.087,112.928,21.663,72.158,63.742,42.827,65.176,21.070,38.900,58.007,17.798,27.834,2.119,45.757,38.494,71.161,95.411,
                  46.412,24.468,79.140,41.954,41.954,73.966,28.863,42.827,18.857,21.070,58.007,50.152,4.270,95.411,71.005,58.007,79.140,58.100,58.007,63.742,592,5.672,5.548,498,4.581,7.387,10.753,467,467,4.581,5.548,467,748,436,4.581,4.581,4.581,592,4.581,592,3.927,467,3.646,4.581,
                  5.454,5.454,2.961,249,2.711,2.618,2.711,2.555,2.961,2.961,2.524,2.306,374,24.530,2.711,792.961,374,249,2.61,2.618,3.085,2.524,24.530,2.618,2.929,2.929,25.746,1.714,3.085,3.085,2.742,1.589,1.620,2.618,1.683,1.589,2.961,25.746,3.085,2.961,1.683,1.683,1.714,2.524,
                  2.275,1.683,2.587,1.714,1.994,1.589,24.530,3.272,1.776,1.620,3.272,1.589,1.496,1.807,1.464,2.337,751.807,1.620,1.589,1.776,1.776,2.337,1.776,2.337,2.026,24.530,997,1.620,1.589,1.776,1.807,2.337,21.164,1.776,1.433,13.808,1.776,22.224,1.776,2.524,1.776,1.433,46.100,
                  4.239,21.164,1.776,2.929,13.808,13.808,2.929,2.929,13.808,21.164,2.368,2.929,2.929,1.433,1.776,13.808,2.929,21.164,2.929,2.929,46.100,1.776,1.776,2.929,13.808,4.239,2.929,13.808,2.929,22.224,22.224,2.368,2.929,2.929,21.164,2.929,1.433,2.368,4.239,3.085,21.257,3.085,
                  3.085,16.675,4.239,4.239,4.239,218,4.239,18.608,1.807,2.400,218,2.711,218,2.929,1.807,311,311,311,311,2.337,2.119,4.239,4.239,4.239,2.618,4.239,1.839,2.368,436,3.678,1.839,2.649,2.462,2.337,4.239,4.239,1.994,1.901,2.618,3.085,2.119,40.801,3.615,1.839,342,3.896,1.839]

PrecoVendaMedia = [10.442, 14.806,22.598,8.883,1.434,28.832,21.040,2.026,27.274,23.222,8.883,21.351,2.338,1.901,7.013,24.157,22.598, 24.157,10.130,8.260,8.883,1.527,1.371,779,904,94, 12.936,187,6.078,23.222,966,7.325,15.429,187,1.714,9.195,14.494,1.091,94,7.013,18.234,7.948,6.078,1.714,3.896,
                   2.338,8.260,3.273,1.808,24.157,4.644,717,561,499,8.572,1.122,1.403,14.806,1.278,62.309,21.663,1.403,58.444,7.325,1.029,14.806,61.561,28.832,13.559,44.417,12.312,1.403,23.222,8.260,1.527,12.312,1.714,468,16.364,4.520,45.913,14.806,717,3.896,10.878,779,1.091,1.839,10.754,14.806,1.184,
                   35.066,10.941,717,125,18.234,64.678,530,21.975,30.079,5.455,530,5.143,4.520,4.208,4.208,4.208,3.896,374,374,374,3.585,3.585,3.585,3.585,3.460,343,343,343,3.273,3.273,3.273,3.273,31,2.961,281,281,2.338,249,2.431,2.338,2.338,2.338,2.244,2.026,2.026,2.026,2.026,187,187,187,187,187,187,
                   187,187,1.714,1.714,1.621,1.590,156,156,156,156,156,1.496,1.496,1.496,1.496,1.403,1.403,1.403,1.403,1.403,1.403,1.403,1.403,1.309,1.309,125,125,125,125,125,1.184,1.184,1.091,1.091,1.091,966,94,94,94,842,779,779,779,779,779,62, 62,62,62,62,62,561,530,499,468,374,31,10.130,1.371,
                   9.195,8.572,16.364,17.923,16.053,2.462,15.117,966,36.625,35.066,904,16.364,1.403,904,9.819,20.105,1.403,1.091,1.403,187,25.715,15.928,842,16.364,7.948,15.429,966,19.170,28.832,35.690,1.216,1.714,2.836,966,35.066,1.496,62,16.676,14.806,1.371,19.481,18.546,1.621,11.689,18.546,
                   125,16.364,4.021,156,1.683,2.244,16.364,94,31.949,2.649,2.618,1.216,28.521,1.714,125,2.057,125,2.026,11.377,26.027,1.496,2.088,1.278,94,2.338,7.013,1.652,3.398,26.962,3.023,187,19.481,16.364,655,25.715,28.022,1.091,2.307,17.611,17.923,2.618,31.513,1.403,1.683,1.995,10.130,11.689,26.650,2.961,125,10.442,3.585,1.652]

KmsRodadoMedia = [13500,21500,3450,2600,21225,1035,9398,16714,10136,21183,1067,25500.7500,13000,38713,21500,20839,21500,17750,20721,12500,1200,25000,22640,28439,10000,27569,8100,22271,22500,25719,27100,19500,22500,22500,24999,24383,63500,5039,31000,12262,23353,29000,22890,25000,7500,32266,32500,12935,
                  18500,52353,20000,7500,67500,45000,35000,20267,25000,19742,20500,20000,20294,39000,23500,3000,22500,5500,29500,44000,6000,35500,22500,28000,21500,41500,18000,36000,67577,40000,44500,11500,20000,7500,19000,98588,71000,39000,28000,23500,20000,31000,29121,37500,20000,44500,
                  36000,14500,4350,22500,25012,1500,700,2000,600,2050,10850,8250,7500,9000,5500,3000,4350,3500,17500,8500,8750,16500,7000,13000,2700,2850,3450,3000,23250,5750,20000,650,3500,1500,2500,5500,9000,1750,250,5900,2500,11750,8000,7500,8300,16000,10000,14500,12500,
                  12500,9500,7500,29000,22500,12000,3000,15500,6500,22500,4000,2150,7500,11500,4300,2000,12000,11500,7250,13500,7000,250,500,21000,6000,7000,2750,3350,6850,650,19300,37500,15000,12000,9500,106500,30000,25000,15000,10500,13000,950,11000,16000,9000,27500,30000,12500,24500,12000,25000,17500,250000,16500,
                  17500,26500,46116,29000,14100,26730,14141,1746,6239,17398,1717,10562,17887,21767,11335,15802,10057,18050,6250,7500,22539,18000,19244,16000,38816,30690,18099,11258,12339,28500,30000,26066,22500,7500,6450,26500,2246,7570,5924,34000,30120,11854,16161,17933,17000,3500,24500,
                  35500,17500,18000,15000,8500,17967,28350,15713,24000,27121,26837,24781,20162,12500,18027,14611,2800,20011,8001,20013,10600,17500,9717,9500,9414,34670,34781,13800,30601,8250,15376,12400,10890,2000,20063,7232,25228,31500,4505,4900,7529,14284,22000,17000,5490,9500,15713,6000,19000,16509,30038,16994,30000,43967,4500,2732]

# Analisando os dados do PrecoAtualMedia
# podemos perceber que a média está muito longe da mediana o que
# sugere um desvio padrão alto e uma distribuição anormal dos dados.
PrecoAtulMedia = PrecoAtualMedia
a=pd.Series(PrecoAtualMedia).mean()
b=pd.Series(PrecoAtualMedia).median()
c=pd.Series(PrecoAtualMedia).std()
print('Media = ',a)
print('Mediana = ',b)
print('Desvio Padrão = ',c)

# Analisando os dados do PrecoVendaMedia
# podemos perceber que a média está muito longe da mediana o que
# sugere um desvio padrão alto e uma distribuição anormal dos dados.
PrecoVendaMedia = PrecoVendaMedia
a=pd.Series(PrecoVendaMedia).mean()
b=pd.Series(PrecoVendaMedia).median()
c=pd.Series(PrecoVendaMedia).std()
print('Media = ',a)
print('Mediana = ',b)
print('Desvio Padrão = ',c)

# Analisando os dados do KmsRodadoMedia
# podemos perceber que a média está muito proxima da mediana o que
# sugere um desvio padrão alto e uma distribuição normal dos dados.
KmsRodadoMedia = KmsRodadoMedia
a=pd.Series(KmsRodadoMedia).mean()
b=pd.Series(KmsRodadoMedia).median()
c=pd.Series(KmsRodadoMedia).std()
print('Media = ',a)
print('Mediana = ',b)
print('Desvio Padrão = ',c)

#### gráfico
plt.figure(figsize=(12, 8))
plt.title('Correlação entre kmd rodados e valor de venda atual do automóvel')
plt.xlabel('kms_rodado')
plt.ylabel('venda_atu')

#correlação que será analisada
plt.scatter(KmsRodadoMedia, PrecoAtualMedia)
plt.show()

# Descobrindo a venda atual usando Machine Learning
# Veja que temos uma reta muito parecida com a que traçamos anteriormente, só que nesse caso tivemos menos trabalho e
# não precisamos criar nada de forma manual para tentar descobrir qual é a função para descobrir o valor atual de venda.
from sklearn import linear_model

reg = linear_model.LinearRegression()

reg.fit(carros["kms_rodado"].values.reshape(-1,1),carros["preco_atu"])

reg.coef_

reg.intercept_

plt.scatter(carros["kms_rodado"],carros["preco_atu"])
x = np.array(carros["kms_rodado"])
y = reg.intercept_ + x*reg.coef_
plt.plot(x,y,"r")
plt.show()

# Inserindo a informação no gráfico para verificar onde ela vai cruzar nossa linha
reg.predict([[75]])

plt.scatter(carros["kms_rodado"],carros["preco_atu"])
plt.scatter(75,reg.predict([[224000]])[0],color="k")
x = np.array(carros["kms_rodado"])
y = reg.intercept_ + x*reg.coef_
plt.plot(x,y,"r")
plt.show()

reg.predict([[224000]])

plt.scatter(carros["kms_rodado"],carros["preco_atu"])
plt.scatter(75,reg.predict([[224000]])[0],color="k")
x = np.array(carros["preco_atu"])
y = reg.intercept_ + x*reg.coef_
plt.plot(x,y,"r")
plt.show()

# Função (plotar_histograma): Gera um histograma
def plotar_histograma(lista_dados,eixoX,titulo,cor,barras):
  fig, ax = plt.subplots(1, 1, figsize=(10, 8))
  fig.set_facecolor("white") # cor de fundo

  plt.hist(lista_dados,
           bins=barras,
           density=False,
           histtype='bar',
           color=cor,
          edgecolor='k')

  #exibição do gráfico
  ax.set_title(titulo)
  ax.set_ylabel("Frequencias de " +eixoX)
  ax.set_xlabel(eixoX)
  plt.show()
# esta função não irá retornar nenhum valor

# Função (plotar_boxplot): Gera um boxplot
def plotar_boxplot(lista_dados,titulo,atributoCalculo,cor):
  fig, ax = plt.subplots(1, 1, figsize=(10, 8))
  fig.set_facecolor("white") # cor de fundo
  ax = sns.boxplot(x=lista_dados, boxprops=dict(facecolor=cor));
  plt.xlabel(titulo, labelpad=14);
  ax.axes.set_title(atributoCalculo, fontsize=20, y=1.01);
# esta função não irá retornar nenhum valor

plotar_histograma(PrecoAtulMedia,'preco_atu','Distribuição da Média do Preço Atual dos carros [2003 a 2018]','#297c00',7)

plotar_histograma(PrecoVendaMedia,'preco_venda','Distribuição da Média do Preço Atual dos carros [2003 a 2018]','#FFF92E',7)

plotar_histograma(KmsRodadoMedia,'kmd_rodado','Distribuição da Média do Preço Atual dos carros [2003 a 2018]','#0D2DFF',7)

# Histrograma de comparação dos três média
#configuração do gráfico
fig, ax = plt.subplots(1, 1, figsize=(10, 8))
fig.set_facecolor("white") # cor de fundo
plt.title('Preço Atual X Preço Venda X KMS Rodado - Entre 2003 a 2018')
plt.xlabel('Média de Idade')
plt.ylabel('Frequência por Idade')
#plotagem
plt.hist(PrecoAtulMedia,alpha=0.4, label='preco_atu', color='#297c00')
plt.hist(PrecoVendaMedia,alpha=0.4, label='preco_venda', color='#FFF92E')
plt.hist(KmsRodadoMedia,alpha=0.4, label='kms_rodado', color='#0D2DFF')
plt.legend();

plotar_boxplot(PrecoAtulMedia,'Distribuição da Média [2003 a 2018]','preco_atu','#297c00')

plotar_boxplot(PrecoVendaMedia,'Distribuição da Média [2003 a 2018]','preco_venda','#FFF92E')

plotar_boxplot(KmsRodadoMedia,'Distribuição da Média [2003 a 2018]','kms_rodado','#0D2DFF')

"""# Machine Learning com Linguagem Python e Power BI Dentro do Colab"""

# Importar Bibliotecas
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Carrega os dados
carros = pd.read_excel('preco-carros.xlsx')

type(carros)

# Visualiza as 10 primeiras linhas
carros.head(10)

# Resumo estatístico
carros[['preco_atu', 'preco_venda', 'kms_rodado']].describe()

# Cria o padronizador dos dados
padronizador = StandardScaler()

# Aplica o padronizador somente nas colunas de interesse
dados_padronizados = padronizador.fit_transform(carros[['preco_atu', 'preco_venda', 'kms_rodado']])

# Visualiza os dados
print(dados_padronizados)

# Definimos o número de clusters (k)
k = 3

# Criamos o modelo K-means
kmeans = KMeans(n_clusters = k)

# Treinamento do modelo com os dados padronizados
kmeans.fit(dados_padronizados)

# Atribuímos os rótulos dos clusters aos clientes
carros['cluster'] = kmeans.labels_

# Exibe o resultado (10 primeiras linhas)
carros.head(10)

# Salvamos o resultado em disco
carros.to_excel('segmentos.xlsx', index = False)

# Instala o pacote
!pip install -q powerbiclient

# Carrega as funções usadas para autenticar e gerar  relatórios
from powerbiclient import QuickVisualize, get_dataset_config, Report
from powerbiclient.authentication import DeviceCodeLoginAuthentication

# Define a autenticação no Power BI Service
device_auth = DeviceCodeLoginAuthentication()

# Cria o relatório no Power BI
relatorio_PBI = QuickVisualize(get_dataset_config(carros), auth = device_auth)

# Renderiza (Visualiza o relatório)
relatorio_PBI

from google.colab import output
output.enable_custom_widget_manager()

from google.colab import output
output.disable_custom_widget_manager()

from google.colab import output
output.enable_custom_widget_manager()

from google.colab import output
output.disable_custom_widget_manager()



"""# Fim do Projeto"""